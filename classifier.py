# -*- coding: utf-8 -*-
"""Classifier

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1br6BrDnxBXmw4-btVxYO2focaKjW96ON

#  Clasificador de perros y gatos para imágenes

## Descarga de módulos a utilizar
"""

!pip install -q -U keras-tuner

!pip install tensorflowjs

import cv2
import keras
import numpy as np
import tensorflow as tf
import keras_tuner as kt
import tensorflowjs as tfjs
import matplotlib.pyplot as plt
import tensorflow_datasets as tfds
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Input, LeakyReLU, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.preprocessing.image import ImageDataGenerator

"""## Preprocesamiento de los datos

- Descarga del set de datos de perros y gatos
"""

datos, metadatos = tfds.load('cats_vs_dogs', as_supervised=True, with_info=True)

"""- Impresión de los metadatos para revisarlos"""

metadatos

"""- Tamaño de imagen a utilizar en los modelos resultantes para este dataset, en este ejemplo usaremos un tamaño de 80x80 para acelerar un poco los modelos en comparación con el modelo de ejemplo que nos dio."""

TAMANO_IMG=80

plt.figure(figsize=(9,9))
for i, (imagen, etiqueta) in enumerate(datos['train'].take(9)):
  imagen = cv2.resize(imagen.numpy(), (TAMANO_IMG, TAMANO_IMG))
  imagen = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)
  plt.subplot(3, 3, i+1)
  plt.xticks([])
  plt.yticks([])
  plt.imshow(imagen, cmap='gray')
plt.savefig("Clasificador_Perros_Gatos/Ejemplos_ajustados_dataset.jpg")
plt.close()

#Variable que contendra todos los pares de los datos (imagen y etiqueta) ya modificados (blanco y negro, 80x80)
datos_entrenamiento = []

for i, (imagen, etiqueta) in enumerate(datos['train']): #Todos los datos
  imagen = cv2.resize(imagen.numpy(), (TAMANO_IMG, TAMANO_IMG))
  imagen = cv2.cvtColor(imagen, cv2.COLOR_BGR2GRAY)
  imagen = imagen.reshape(TAMANO_IMG, TAMANO_IMG, 1) #Cambiar tamano a 80,80,1
  datos_entrenamiento.append([imagen, etiqueta])

#Ver los datos del primer indice
datos_entrenamiento[0]

#Ver cuantos datos tengo en la variable
len(datos_entrenamiento)

#Preparar mis variables X (entradas) y y (etiquetas) separadas

X = [] #imagenes de entrada (pixeles)
y = [] #etiquetas (perro o gato)

for imagen, etiqueta in datos_entrenamiento:
  X.append(imagen)
  y.append(etiqueta)

X

#Normalizar los datos de las X (imagenes). Se pasan a numero flotante y dividen entre 255 para quedar de 0-1 en lugar de 0-255
X = np.array(X).astype(float) / 255

y

# Convertir etiquetas en arreglo simple
y = np.array(y)

X.shape

"""- Verificación de posibles clases desbalanceadas:"""

print(f"Porcentaje de cada clase enel dataset:\nGatos - {round(100*np.sum(y == 0)/y.shape[0],2)}%,\nPerros - {round(100*np.sum(y == 1)/y.shape[0],2)}%")

"""Dado que el porcentaje de cada clase es de casi el 50%, no es necesario aplicar ninguna tecnica para clases desbalanceadas en este dataset

- Partición de los datos en conjuntos de entrenamiento, de validación y de prueba mantienendo una proporción de los datos totales de un 80% y 20% en cada conjuntos, respectivamente
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
X_train.shape, X_test.shape

"""# Buscando los mejores modelos

## Modelo regular
"""

def build_model(hp):
    """
    Construye y compila una red neuronal utilizando Keras, con hiperparámetros optimizados.
    Crea un modelo con una capa de entrada y un número variable de capas ocultas.
    Los hiperparámetros de la red neuronal, como el número de capas, el número de unidades por capa,
    la función de activación y el optimizador, son seleccionados automáticamente utilizando el
    parámetro `hp`, que permite realizar una búsqueda de hiperparámetros.

    Args:
    hp: Un objeto de la librería KerasTuner que facilita la búsqueda de hiperparámetros.
        Este objeto es utilizado para seleccionar el número de capas, el número de unidades por capa,
        la función de activación y el optimizador.

    Returns:
    model: Un modelo de red neuronal compilado y listo para entrenarse.
    """
    model = Sequential()
    model.add(Input(shape=(80, 80, 1)))
    model.add(Flatten())

    num_blocks = hp.Int("num_dense_blocks", min_value=2, max_value=4)
    unit_options = [64, 128, 256, 512]

    for i in range(num_blocks):
        units = hp.Choice(f"dense_{i+1}_units", unit_options[:len(unit_options) - (3 - i)])
        model.add(Dense(units, activation='relu'))
        model.add(BatchNormalization())
        model.add(Dropout(hp.Float(f'dropout_{i+1}', min_value=0.2, max_value=0.5, step=0.1)))

    model.add(Dense(80, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))

    lr = hp.Choice('lr', values=[1e-3, 1e-4])
    optimizer_choice = hp.Choice("optimizer", values=["Adam", "SGD", "Adagrad"])
    optimizers_dict = {
        "Adam": tf.keras.optimizers.Adam(learning_rate=lr),
        "SGD": tf.keras.optimizers.SGD(learning_rate=lr),
        "Adagrad": tf.keras.optimizers.Adagrad(learning_rate=lr)
    }

    model.compile(optimizer=optimizers_dict[optimizer_choice],
                  loss="binary_crossentropy",
                  metrics=["accuracy"])

    return model

"""Hyperband para búsqueda de hiperparámetros en la red regular"""

tuner = kt.Hyperband(
    build_model,
    objective=kt.Objective("val_accuracy", "max"),
    executions_per_trial=1,
    max_epochs=5,
    factor=3,
    directory='salida',
    project_name='intro_to_HP',
    overwrite=True,
)

early_stop = EarlyStopping(monitor = 'val_loss', patience = 3, verbose = 1, restore_best_weights = True)
hist = tuner.search(X_train, y_train, validation_split = 0.25, callbacks = [early_stop], verbose = 0)
best_hp = tuner.get_best_hyperparameters()[0]
best_model = tuner.hypermodel.build(best_hp)

"""Entrenamiento del mejor modelo regular obtenido"""

hist = best_model.fit(X_train, y_train, validation_split = 0.25, epochs = 50, callbacks=[early_stop], verbose = 0)

"""## Modelo CNN"""

def build_model_cnn(hp):
    """
    Construye y compila una red neuronal convulocional utilizando Keras, con hiperparámetros optimizados.
    Crea un modelo con una capa de entrada y un número variable de capas ocultas.
    Los hiperparámetros de la red neuronal, como el número de capas, el número de unidades por capa,
    la función de activación y el optimizador, son seleccionados automáticamente utilizando el
    parámetro `hp`, que permite realizar una búsqueda de hiperparámetros.

    Args:
    hp: Un objeto de la librería KerasTuner que facilita la búsqueda de hiperparámetros.
        Este objeto es utilizado para seleccionar el número de capas, el número de unidades por capa,
        la función de activación y el optimizador.

    Returns:
    model: Un modelo de red neuronal compilado y listo para entrenarse.
    """
    model = Sequential()
    model.add(Input(shape=(80, 80, 1)))
    num_blocks = hp.Int("num_conv_blocks", min_value=2, max_value=4)
    filter_options = [32, 64, 128, 256]

    for i in range(num_blocks):
        filters = hp.Choice(f"conv_{i+1}_filters", filter_options[:len(filter_options) - (3 - i)])
        model.add(Conv2D(filters=filters, kernel_size=(3, 3), activation='relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(BatchNormalization())
        model.add(Dropout(hp.Float(f'dropout_{i+1}', min_value=0.2, max_value=0.5, step=0.1)))

    model.add(Flatten())
    model.add(Dense(80, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))

    lr = hp.Choice('lr', values=[1e-3, 1e-4])
    optimizer_choice = hp.Choice("optimizer", values=["Adam", "SGD", "Adagrad"])
    optimizers_dict = {
        "Adam": tf.keras.optimizers.Adam(learning_rate=lr),
        "SGD": tf.keras.optimizers.SGD(learning_rate=lr),
        "Adagrad": tf.keras.optimizers.Adagrad(learning_rate=lr)
    }

    model.compile(optimizer=optimizers_dict[optimizer_choice],
                  loss="binary_crossentropy",
                  metrics=["accuracy"])
    return model

"""Hyperband para búsqueda de hiperparámetros en la red convolucional"""

tuner_cnn = kt.Hyperband(
    build_model,
    objective=kt.Objective("val_accuracy", "max"),
    executions_per_trial=1,
    max_epochs=5,
    factor=3,
    directory='salida',
    project_name='intro_to_HP',
    overwrite=True,
)

early_stop_cnn = EarlyStopping(monitor = 'val_loss', patience = 3, verbose = 1, restore_best_weights = True)
hist_cnn = tuner_cnn.search(X_train, y_train, validation_split = 0.25, callbacks = [early_stop_cnn], verbose = 0)
best_hp_cnn = tuner_cnn.get_best_hyperparameters()[0]
best_model_cnn = tuner_cnn.hypermodel.build(best_hp_cnn)

"""Entrenamiento del mejor modelo convolucional obtenido"""

hist_cnn = best_model_cnn.fit(X_train, y_train, validation_split = 0.25, epochs = 50, callbacks=[early_stop], verbose = 0)

"""## Gráficas de los resultados"""

def plot_hist(hist, label = ''):
    """
    Gráfica de la precisión (accuracy) del modelo durante el entrenamiento y la validación.

    Args:
    hist (History): Objeto de historial de Keras que contiene los valores de precisión
                    durante el entrenamiento. Este objeto se obtiene al entrenar un modelo.
    label (str): Etiqueta adicional que se utilizará para nombrar el archivo de la gráfica guardada.
                 Valor por defecto: ''

    Returns:
    None: La función guarda la gráfica como un archivo PNG y no devuelve nada.
    """
    plt.figure()
    plt.plot(hist.history["accuracy"])
    plt.plot(hist.history["val_accuracy"])
    plt.title("Model accuracy")
    plt.ylabel("accuracy")
    plt.xlabel("epoch")
    plt.legend(["train", "validation"], loc="upper left")
    plt.ylim((0,1.1))
    plt.grid()
    plt.savefig('Clasificador_Perros_Gatos/Model_Accuracy_'+label+'.png')
    plt.close()

def plot_hist_loss(hist, label = ''):
    """
    Gráfica de la pérdida (loss) del modelo durante el entrenamiento y la validación.

    Args:
    hist (History): Objeto de historial de Keras que contiene los valores de pérdida (loss)
                    durante el entrenamiento. Este objeto se obtiene al entrenar un modelo.
    label (str): Etiqueta adicional que se utilizará para nombrar el archivo de la gráfica guardada.
                 Valor por defecto: ''

    Returns:
    None: La función guarda la gráfica como un archivo PNG y no devuelve nada.
    """
    plt.figure()
    plt.plot(hist.history["loss"], color='red')
    plt.plot(hist.history["val_loss"],color = 'blue')
    plt.title("Model loss")
    plt.ylabel("loss")
    plt.xlabel("epoch")
    plt.legend(["train", "validation"], loc="upper left")
    plt.grid()
    plt.savefig('Clasificador_Perros_Gatos/Model_Loss_'+label+'.png')
    plt.close()

"""- Modelo regular"""

historial = best_model.fit(X_train, y_train, validation_split = 0.25, epochs = 50, callbacks=[early_stop], verbose = 0)

plot_hist(historial)

plot_hist_loss(historial)

"""- Modelo CNN"""

historial_cnn = best_model_cnn.fit(X_train, y_train, validation_split = 0.25, epochs = 50, callbacks=[early_stop_cnn], verbose = 0)

plot_hist(historial_cnn, 'cnn')

plot_hist_loss(historial_cnn, 'cnn')

"""## Modelos a usar en la app"""

best_model.save('Clasificador_Perros_Gatos/model.h5')
tfjs.converters.save_keras_model(best_model, "Clasificador_Perros_Gatos/model_tfjs")
best_model_cnn.save('Clasificador_Perros_Gatos/model_cnn.h5')
tfjs.converters.save_keras_model(best_model_cnn, "Clasificador_Perros_Gatos/model_cnn_tfjs")